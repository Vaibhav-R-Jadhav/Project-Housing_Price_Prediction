{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b14f79c3-60ce-4d90-b0ec-7e69bd4bb186",
    "_uuid": "94b4ddb51d694c35dbab357788b7e5c4517ecc39"
   },
   "source": [
    "#### 1. CRIM - per capita crime rate by town\n",
    "#### 2. ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "#### 3. INDUS - proportion of non-retail business acres per town.\n",
    "#### 4. CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "#### 5. NOX - nitric oxides concentration (parts per 10 million)\n",
    "#### 6. RM - average number of rooms per dwelling\n",
    "#### 7. AGE - proportion of owner-occupied units built prior to 1940\n",
    "#### 8. DIS - weighted distances to five Boston employment centres\n",
    "#### 9. RAD - index of accessibility to radial highways\n",
    "#### 10. TAX - full value property tax rate per $10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. PTRATIO - pupil-teacher ratio by town\n",
    "#### 12. B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "#### 13. LSTAT - % lower status of the population\n",
    "#### 14. MEDV - Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "data = pd.read_csv(r\"C:\\Users\\Vaibhav\\OneDrive\\Documents\\Decode\\Project-Housing_Price_Prediction\\housing_price_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From get-go,  two data coulmns show interesting summeries. They are\n",
    "* ZN (proportion of residential land zoned for lots over 25,000 sq.ft.)  with 0 for 25th, 50th percentiles.\n",
    "* Second, CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise) with 0 for 25th, 50th and 75th percentiles.\n",
    "* These summeries are understandable as both variables are conditional as well as categorical variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First assumption would be that these coulms may not be useful in regression task such as predicting MEDV (Median value of owner-occupied homes).\n",
    "* Another interesing fact on the dataset is the max value of MEDV. From the original data description, it says: Variable #14 seems to be censored at 50.00 (corresponding to a median price of $50,000).\n",
    "* Based on that, values above 50.00 may not help to predict MEDV. Let's plot the dataset and see interesting trends/stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "23d0dfd5-b7a2-46e4-baed-10ca76a62dbc",
    "_uuid": "50fd4b0697c8c6f9e30c6caa3f60c7d3a03d5a3d"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k,v in data.items():\n",
    "    sns.boxplot(y=k, data=data, ax=axs[index])\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e73e8b3a-daf7-49fa-8611-f3ce9a1b2c9c",
    "_uuid": "4d4f0c23bb7761cfb67761df216a8b1bc2e20f75"
   },
   "source": [
    "* Columns like CRIM, ZN, RM, B seems to have outliers.\n",
    "* Let's see the outliers percentage in every column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3db29b4c-9c8c-4457-8064-91d6c3b5ed50",
    "_uuid": "b80e456c7039e0d5c1c3f61e33cb8041ded81622"
   },
   "outputs": [],
   "source": [
    "for k, v in data.items():\n",
    "    q1 = v.quantile(0.25)\n",
    "    q3 = v.quantile(0.75)\n",
    "    irq = q3 - q1\n",
    "    v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n",
    "    perc = np.shape(v_col)[0] * 100.0 / np.shape(data)[0]\n",
    "    print(f\"Column {k} outliers = {round(perc,2)}%\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try removing outliers from all the coulmns and then re-plot distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = [\"CRIM\",\"ZN\",\"RM\",\"B\"]\n",
    "# fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n",
    "# index = 0\n",
    "# axs = axs.flatten()\n",
    "# for c in data.columns:\n",
    "#     if(c in col):\n",
    "#         percentile25 = data[c].quantile(0.25)\n",
    "#         percentile75 = data[c].quantile(0.75)\n",
    "#         iqr = percentile75-percentile25\n",
    "#         upper_limit = percentile75+(1.5*iqr)\n",
    "#         lower_limit = percentile25-(1.5*iqr)\n",
    "#         data = data[data[c]<=upper_limit]\n",
    "#         data = data[data[c]>=lower_limit]\n",
    "#         plt.figure()  #for not to overlap other graphs\n",
    "#         sns.boxplot(y=c, data=data, ax=axs[index])\n",
    "#         index += 1\n",
    "#     else:\n",
    "#         plt.figure()  #for not to overlap other graphs\n",
    "#         sns.boxplot(y=c, data=data, ax=axs[index])\n",
    "#         index += 1\n",
    "# plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Null values with mean or median, just to reserve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"INDUS\"].fillna(data[\"INDUS\"].mean(), inplace = True)\n",
    "# data[\"AGE\"].fillna(data[\"AGE\"].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"CHAS\"].fillna(data[\"CHAS\"].median(), inplace = True)\n",
    "# data[\"LSTAT\"].fillna(data[\"LSTAT\"].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4f07967-903d-47ea-9bec-6153e8b18446",
    "_uuid": "d75be26652e9370e490a535db7433f636767a1a8"
   },
   "source": [
    "Let's see how these features plus MEDV distributions looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3365b6f2-14dc-4ec3-9d6b-b5ea48b62971",
    "_uuid": "ba686a43a8c707f42259c3254cb028ff97d0d104"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k,v in data.items():\n",
    "    sns.distplot(v, ax=axs[index])\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a7c16fb-3c18-4d76-8ceb-602f43b90aef",
    "_uuid": "a0a4b0a6a28538e9ad4df92da49856f599c25383"
   },
   "source": [
    "The histogram also shows that columns CRIM, ZN, B has highly skewed distributions. Also MEDV looks to have a normal distribution (the predictions) and other colums seem to have norma or bimodel ditribution of data except CHAS (which is a discrete variable).\n",
    "\n",
    "Now let's plot the pairwise  correlation on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "de1f6ba3-2aab-43ea-ab58-3f938b111ab5",
    "_uuid": "a03fc465f35ebb73358874376569f2fe856c2763"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(data.corr().abs(),  annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "938c8cce-a377-4450-be3d-9d56e2b10f25",
    "_uuid": "6740116517c45740b4c60b2626b6eb477051a52c"
   },
   "source": [
    "From correlation matrix, we see TAX and RAD are highly correlated features. The columns LSTAT, INDUS, RM, TAX, NOX, PTRAIO has a correlation score above 0.5 with MEDV which is a good indication of using as predictors. Let's plot these columns against MEDV. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Null values with median for reserving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']]\n",
    "y = data[\"MEDV\"]\n",
    "for c in x.columns:\n",
    "    x[c] = x[c].fillna(x[c].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting each column against MEDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sets = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\n",
    "fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for i, k in enumerate(column_sets):\n",
    "    sns.regplot(y=y, x=x[k], ax=axs[i])\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As the data points are skewed at one side in most of the columns we will have to do normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.3,random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "for c in x.columns:\n",
    "    xtrain[c] = sc.fit_transform(xtrain[[c]])\n",
    "\n",
    "for c in xtest.columns:\n",
    "     xtest[c] = sc.fit_transform(xtest[[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x2 = x1.drop(\"MEDV\", axis = 1).values\n",
    "# y2 = x1[\"MEDV\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "54680e1a-addd-4a28-aac1-3065ecf941d2",
    "_uuid": "321b79669416c5e71541539bb10e7c115e78e8ea"
   },
   "outputs": [],
   "source": [
    "# So with these analsis, we may try predict MEDV with 'LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE' features.\n",
    "#Let's try to remove the skewness of the data through log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in x.columns:\n",
    "#     plt.figure()\n",
    "#     sns.distplot(x[c])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8355aaeb269fa9f8cf360d86a01754d43111731e"
   },
   "outputs": [],
   "source": [
    "# #y =  np.log1p(y)\n",
    "# for col in x.columns:\n",
    "#     if np.abs(x[col].skew()) > 0.3:\n",
    "#         x[col] = np.log1p(x[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f10db586d1b7f15a5eeb441de373210790c41729"
   },
   "source": [
    "# Let's try Linear Regression on dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(xtrain,ytrain)\n",
    "ypred = lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "Linear_Regression = r2_score(ytest,ypred)\n",
    "print(\"r2 score = \",r2_score(ytest,ypred))\n",
    "print(\"MSE = \",mean_squared_error(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training score = \",lr.score(xtrain,ytrain))\n",
    "print(\"testing score = \",lr.score(xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets try Ridge Regression on same data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rg = Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before fitting model, lets do hyper parameter tuning for getting best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "para = {\"alpha\":[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,40,80,100]}\n",
    "grd = GridSearchCV(rg,para,scoring = \"neg_mean_squared_error\", cv = 5)\n",
    "grd.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grd.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = Ridge(alpha = 1)\n",
    "rg.fit(xtrain,ytrain)\n",
    "ypred = rg.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "Ridge_Regression = r2_score(ytest,ypred)\n",
    "print(\"r2 score = \",r2_score(ytest,ypred))\n",
    "print(\"MSE = \",mean_squared_error(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training score = \",rg.score(xtrain,ytrain))\n",
    "print(\"testing score = \",rg.score(xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So Ridge is giving better score as compare to Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets try for anather regularization technique i.e Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4c67bb6b-b11e-4da7-906e-93c83ed85c39",
    "_uuid": "4abf281773184265b8e52dad8d58aa72ba41b02a"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "ls = Lasso()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting best parameter for Lasso as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "para = {\"alpha\":[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,40,80,100]}\n",
    "grd = GridSearchCV(ls,para,scoring = \"neg_mean_squared_error\", cv = 5)\n",
    "grd.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grd.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = Lasso(alpha = 0.01)\n",
    "ls.fit(xtrain,ytrain)\n",
    "ypred = ls.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "Lasso = r2_score(ytest,ypred)\n",
    "print(\"r2 score = \",r2_score(ytest,ypred))\n",
    "print(\"MSE = \",mean_squared_error(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training score = \",ls.score(xtrain,ytrain))\n",
    "print(\"testing score = \",ls.score(xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "241956e9-fa40-4524-b6fc-967d39a807ef",
    "_uuid": "d427ce20b9f9e90a27b324b346fe61cabd8a6753"
   },
   "source": [
    "The Liner Regression with and without L2 regularization does not make significant difference in MSE score.\n",
    "Let's try some non parametric regression techniques: SVR, DecisionTreeRegressor, KNeighborsRegressor etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8369e7b-0049-4648-ad23-f5f5d9530cd3",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "3fde7c8a019dcfdfaf60723bde8187923aea2108"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr= SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# para = {'kernel' : ('linear', 'poly', 'rbf', 'sigmoid'),'C' : [1,5,10],'degree' : [3,8],'coef0' : [0.01,10,0.5],'gamma' : ('auto','scale')}\n",
    "# rsv = RandomizedSearchCV(svr,para,scoring = \"neg_mean_squared_error\",n_iter = 5, cv = 5,n_jobs = -1)\n",
    "# rsv.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr= SVR(kernel = 'rbf', gamma = 'scale', degree = 8, coef0 = 0.1, C = 10)\n",
    "svr.fit(xtrain,ytrain)\n",
    "ypred = svr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "SVR = r2_score(ytest,ypred)\n",
    "print(\"r2 score = \",r2_score(ytest,ypred))\n",
    "print(\"MSE = \",mean_squared_error(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training score = \",ls.score(xtrain,ytrain))\n",
    "print(\"testing score = \",ls.score(xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "650b9702-13cb-4364-b12f-f996fa013da4",
    "_uuid": "2adbfea5a7d8b2262e71a6f2e9eed2187ce2576b"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dc = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grd = GridSearchCV(dc, cv = 5, param_grid={\"max_depth\" : [3, 4, 5, 6, 7,8,9]}, scoring='neg_mean_squared_error')\n",
    "grd.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grd.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DecisionTreeRegressor(max_depth =  6)\n",
    "dc.fit(xtrain,ytrain)\n",
    "ypred = dc.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "Decision_Tree_Regressor = r2_score(ytest,ypred)\n",
    "print(\"r2 score = \",r2_score(ytest,ypred))\n",
    "print(\"MSE = \",mean_squared_error(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training score = \",ls.score(xtrain,ytrain))\n",
    "print(\"testing score = \",ls.score(xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0d514db-3056-4a62-a702-979e746073df",
    "_uuid": "ba681bc8dc9405517ed0cfaa2331c1b63b211f73"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grd = GridSearchCV(knn, cv = 5, param_grid={\"n_neighbors\" : [2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n",
    "grd.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grd.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors = 7)\n",
    "knn.fit(xtrain,ytrain)\n",
    "ypred = knn.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "KNeighborsRegressor = r2_score(ytest,ypred)\n",
    "print(\"r2 score = \",r2_score(ytest,ypred))\n",
    "print(\"MSE = \",mean_squared_error(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training score = \",ls.score(xtrain,ytrain))\n",
    "print(\"testing score = \",ls.score(xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "34daccef-02e5-4b57-8f5b-6039ab0bd39f",
    "_uuid": "d1ee1cbb5abca746bc21a0cccb537d5cf2f94f0a"
   },
   "source": [
    "Compared to three models which are chosen through grid search, SVR performes better. Let's try an ensemble method finally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ccee0ea9-9499-4b61-9bbb-f737e7fa2db9",
    "_uuid": "2d6c452ab93d6413688439e100d7368d6019c33c"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para ={'n_estimators':[100, 200], 'learning_rate': [0.1,0.05,0.02], 'max_depth':[2, 4,6], 'min_samples_leaf':[3,5,9]}\n",
    "grd = GridSearchCV(gbr, cv = 5, param_grid = para, scoring='neg_mean_squared_error',n_jobs = -1)\n",
    "grd.fit(xtrain,ytrain)\n",
    "grd.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(alpha=0.9,learning_rate=0.1, max_depth=2, min_samples_leaf=3, min_samples_split=2, n_estimators=200, random_state=30)\n",
    "gbr.fit(xtrain,ytrain)\n",
    "ypred = gbr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "Gradient_Boosting_Regressor = r2_score(ytest,ypred)\n",
    "print(\"r2 score = \",r2_score(ytest,ypred))\n",
    "print(\"MSE = \",mean_squared_error(ytest,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training score = \",ls.score(xtrain,ytrain))\n",
    "print(\"testing score = \",ls.score(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd261bb0-fcf4-4b9a-951f-0946b467dbc3",
    "_uuid": "28bc1faf45827a4ec68fc72777e96fa84a065001"
   },
   "source": [
    "Let's plot k-fold results to see which model has better distribution of results. Let's have a look at the MSE distribution of these models with k-fold=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8421b80d-d8bd-440f-be5f-0c75cc9a82e6",
    "_uuid": "b65d37fc69ab9b8ced68f0ecc7fe6c11716f39f4"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "# scores_map = pd.DataFrame(scores_map)\n",
    "# sns.boxplot(data=scores_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"Linear_Regression\"] = Linear_Regression\n",
    "df[\"Ridge_Regression\"] = Ridge_Regression\n",
    "df[\"Lasso\"] = Lasso\n",
    "df[\"SVR\"] = SVR\n",
    "df[\"Decision_Tree_Regressor\"] = Decision_Tree_Regressor\n",
    "df[\"KNeighborsRegressor\"] = KNeighborsRegressor\n",
    "df[\"Gradient_Boosting_Regressor\"] = Gradient_Boosting_Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame()\n",
    "\n",
    "new = {\"Linear_Regression\" : Linear_Regression,\n",
    "\"Ridge_Regression\" : Ridge_Regression,\n",
    "\"Lasso\" : Lasso,\n",
    "\"SVR\" : SVR,\n",
    "\"Decision_Tree_Regressor\" : Decision_Tree_Regressor,\n",
    "\"KNeighborsRegressor\" : KNeighborsRegressor,\n",
    "\"Gradient_Boosting_Regressor\" : Gradient_Boosting_Regressor\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(new, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"Model\" , \"R2_Score\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"R2_Score\"]==df[\"R2_Score\"].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So we come to Conclusion that SVM's SVR is the best model for our Data Set with the highest accuracy . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a16fe973-bbdd-4d4c-a14a-8ab357b9c918",
    "_uuid": "7b6c29011f2e0791ed9239b760eec31fc2f1c37e"
   },
   "source": [
    "### Load Pickled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"trained_model_svr.sav\"\n",
    "pk.dump(svr, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pk.load(open(\"trained_model_svr.sav\", \"rb\"))\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpp(input_data):\n",
    "    # input_data = (4.98, 2.31, 0.538, 15.3, 6.575, 296, 4.09, 65)\n",
    "    arr = np.asarray(input_data)\n",
    "    arr = arr.reshape(1, -1)\n",
    "    ar = sc.fit_transform(arr)\n",
    "\n",
    "    prediction = loaded_model.predict(ar)\n",
    "    #res = (\"The Median value of owner-occupied homes in $1000's is \", prediction)\n",
    "    return(f\"The Median value of owner-occupied homes in $1000's is {round(prediction[0],2)}\")\n",
    "\n",
    "\n",
    "\n",
    "hpp([4.98,2.31,0.538,15.3,6.575,296,4.09,65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
